# sparksql-protobuf

This library provides a way to read parquet file written by SparkSQL back as an RDD of compatible protobuf object.

[![Build Status](https://travis-ci.org/saurfang/sparksql-protobuf.svg?branch=master)](https://travis-ci.org/saurfang/sparksql-protobuf)

## Motivation

SparkSQL is very powerful and easy to use. However it has a few limitations and schema is only detected during runtime
makes developers a lot less confident that they will get things right at first time. This is where protobuf comes in:

1. Protobuf defines nested data structure easily 
2. It doesn't constraint you to the 22 fields limit in case class (no longer true once we upgrade to 2.11+)
3. It is language agnostic and generates code that gives you native object 
hence you get all the benefit of type checking and code completion unlike operating `Row` in Spark/SparkSQL

## Feature
1. `ProtoMessageConverter` has been improved to read from [LIST specification](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#lists)
according to latest parquet documentation. This implementation should be backwards compatible and is able to read repeated
fields generated by writers like SparkSQL.
2. `SettableProtoReadSupport` addresses the need to set explicit protobuf class name as it won't be embedded in the parquet
file at the first palce.
3. `ProtoMessageParquetInputFormat` helps the above process by correctly returning the built protobuf object as value.
4. `ProtoParquetRDD` abstract the Hadoop input format and returns an RDD of your protobuf objects from parquet files directly.

## Usage

In Spark this is as easy as

```scala
val personsPB = new ProtoParquetRDD(sc, "persons.parquet", classOf[Person])
```

where you passes in the `SparkContext`, parquet path and protobuf class.

For more information, please see test cases for illustration.

## Related Work
[Elephant Bird](https://github.com/twitter/elephant-bird)
